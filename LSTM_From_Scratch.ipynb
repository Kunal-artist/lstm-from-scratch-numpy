{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wlf7086sIuP3"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### softmax"
      ],
      "metadata": {
        "id": "ZIwpQ8kmLqZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute softmax values for each column of the input x.\n",
        "\n",
        "    Args:\n",
        "        x -- numpy array of shape (n_y, m)\n",
        "\n",
        "    Returns:\n",
        "        s -- softmax probabilities, same shape as x\n",
        "    \"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=0, keepdims=True))  # for numerical stability\n",
        "    s = e_x / np.sum(e_x, axis=0, keepdims=True)\n",
        "    return s"
      ],
      "metadata": {
        "id": "nk7eDg5DLLKL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Long Short-Term Memory (LSTM) Network"
      ],
      "metadata": {
        "id": "qS1X5XmJOUhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lstm_cell_forward"
      ],
      "metadata": {
        "id": "QeHFK-IWRlY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of x.\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array.\n",
        "\n",
        "    Returns:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "    return s\n"
      ],
      "metadata": {
        "id": "mnGoeUyvXROL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "\n",
        "\n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wf = parameters[\"Wf\"] # forget gate weight\n",
        "    bf = parameters[\"bf\"]\n",
        "    Wi = parameters[\"Wi\"] # update gate weight (notice the variable name)\n",
        "    bi = parameters[\"bi\"] # (notice the variable name)\n",
        "    Wc = parameters[\"Wc\"] # candidate value weight\n",
        "    bc = parameters[\"bc\"]\n",
        "    Wo = parameters[\"Wo\"] # output gate weight\n",
        "    bo = parameters[\"bo\"]\n",
        "    Wy = parameters[\"Wy\"] # prediction weight\n",
        "    by = parameters[\"by\"]\n",
        "\n",
        "    # Retrieve dimensions from shapes of xt and Wy\n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "\n",
        "    # Concatenate a_prev and xt\n",
        "    concat = np.concatenate((a_prev,xt),axis=0)\n",
        "\n",
        "    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas\n",
        "    ft = sigmoid(Wf @ concat + bf)\n",
        "    it = sigmoid(Wi @ concat + bi)\n",
        "    cct = np.tanh(Wc @ concat + bc)\n",
        "    c_next = ft * c_prev + it * cct\n",
        "    ot = sigmoid(Wo @ concat + bo)\n",
        "    a_next = ot * np.tanh(c_next)\n",
        "\n",
        "    # Prediction\n",
        "    yt_pred = softmax(Wy @ a_next + by)\n",
        "\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "    return a_next, c_next, yt_pred, cache"
      ],
      "metadata": {
        "id": "PykE5KUqOT3u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Forward"
      ],
      "metadata": {
        "id": "-cnq9ak8Dnvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_forward(x, a0, parameters):\n",
        "\n",
        "    # Initialize \"caches\", which will track the list of all the caches\n",
        "    caches = []\n",
        "\n",
        "\n",
        "    Wy = parameters['Wy'] # saving parameters['Wy'] in a local variable in case students use Wy instead of parameters['Wy']\n",
        "    # Retrieve dimensions from shapes of x and parameters['Wy']\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "    # initialize \"a\", \"c\" and \"y\" with zeros\n",
        "    a = np.zeros((n_a,m,T_x))\n",
        "    c = np.zeros((n_a,m,T_x))\n",
        "    y = np.zeros((n_y,m,T_x))\n",
        "\n",
        "    # Initialize a_next and c_next\n",
        "    a_next = a0\n",
        "    c_next = np.zeros((n_a,m))\n",
        "\n",
        "    # loop over all time-steps\n",
        "    for t in range(T_x):\n",
        "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
        "        xt = x[:,:,t]\n",
        "        # Update next hidden state, next memory state, compute the prediction, get the cache\n",
        "        a_next, c_next, yt, cache = lstm_cell_forward(xt, a_next, c_next, parameters)\n",
        "        # Save the value of the new \"next\" hidden state in a\n",
        "        a[:,:,t] = a_next\n",
        "        # Save the value of the next cell state\n",
        "        c[:,:,t]  = c_next\n",
        "        # Save the value of the prediction in y\n",
        "        y[:,:,t] = yt\n",
        "        # Append the cache into caches\n",
        "        caches.append(cache)\n",
        "\n",
        "\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return a, y, c, caches"
      ],
      "metadata": {
        "id": "lxzgrXXpDqNe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lstm_cell_backward"
      ],
      "metadata": {
        "id": "2R_DmEZnD1R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_cell_backward(da_next, dc_next, cache):\n",
        "\n",
        "\n",
        "    # Retrieve information from cache\n",
        "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
        "\n",
        "    # Retrieve parameters\n",
        "    Wf = parameters[\"Wf\"]\n",
        "    Wi = parameters[\"Wi\"]\n",
        "    Wc = parameters[\"Wc\"]\n",
        "    Wo = parameters[\"Wo\"]\n",
        "\n",
        "    # Retrieve dimensions\n",
        "    n_x, m = xt.shape\n",
        "    n_a, m = a_next.shape\n",
        "\n",
        "    # Compute gates derivatives\n",
        "    tanh_c_next = np.tanh(c_next)\n",
        "\n",
        "    # Intermediate gradients\n",
        "    dot = da_next * tanh_c_next * ot * (1 - ot)\n",
        "    dcct = (dc_next * it + ot * (1 - tanh_c_next ** 2) * it * da_next) * (1 - cct ** 2)\n",
        "    dit = (dc_next * cct + ot * (1 - tanh_c_next ** 2) * cct * da_next) * it * (1 - it)\n",
        "    dft = (dc_next * c_prev + ot * (1 - tanh_c_next ** 2) * c_prev * da_next) * ft * (1 - ft)\n",
        "\n",
        "    # Compute derivatives of cell state\n",
        "    dc_prev = dc_next * ft + ot * (1 - tanh_c_next ** 2) * ft * da_next\n",
        "\n",
        "    # Concatenate a_prev and xt\n",
        "    concat = np.concatenate((a_prev, xt), axis=0)  # shape: (n_a + n_x, m)\n",
        "\n",
        "    # Compute parameters' gradients\n",
        "    dWf = np.dot(dft, concat.T)\n",
        "    dWi = np.dot(dit, concat.T)\n",
        "    dWc = np.dot(dcct, concat.T)\n",
        "    dWo = np.dot(dot, concat.T)\n",
        "    dbf = np.sum(dft, axis=1, keepdims=True)\n",
        "    dbi = np.sum(dit, axis=1, keepdims=True)\n",
        "    dbc = np.sum(dcct, axis=1, keepdims=True)\n",
        "    dbo = np.sum(dot, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute gradients w.r.t a_prev and xt\n",
        "    d_concat = (\n",
        "        np.dot(Wf.T, dft)\n",
        "        + np.dot(Wi.T, dit)\n",
        "        + np.dot(Wc.T, dcct)\n",
        "        + np.dot(Wo.T, dot)\n",
        "    )\n",
        "\n",
        "    da_prev = d_concat[:n_a, :]\n",
        "    dxt = d_concat[n_a:, :]\n",
        "\n",
        "    # Store gradients in dictionary\n",
        "    gradients = {\n",
        "        \"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev,\n",
        "        \"dWf\": dWf, \"dbf\": dbf,\n",
        "        \"dWi\": dWi, \"dbi\": dbi,\n",
        "        \"dWc\": dWc, \"dbc\": dbc,\n",
        "        \"dWo\": dWo, \"dbo\": dbo\n",
        "    }\n",
        "\n",
        "    return gradients\n"
      ],
      "metadata": {
        "id": "60dzqM60D0W_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Back word\n"
      ],
      "metadata": {
        "id": "KNuii8q5IMH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNGRADED FUNCTION: lstm_backward\n",
        "\n",
        "def lstm_backward(da, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).\n",
        "\n",
        "    Arguments:\n",
        "    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)\n",
        "    caches -- cache storing information from the forward pass (lstm_forward)\n",
        "\n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradient of inputs, of shape (n_x, m, T_x)\n",
        "                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
        "                        dWf, dWi, dWc, dWo -- Gradients w.r.t. gate weight matrices\n",
        "                        dbf, dbi, dbc, dbo -- Gradients w.r.t. gate biases\n",
        "    \"\"\"\n",
        "\n",
        "    (caches_list, x) = caches\n",
        "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches_list[0]\n",
        "\n",
        "    n_a, m, T_x = da.shape\n",
        "    n_x, m = x1.shape\n",
        "\n",
        "    # Initialize gradients with zeros\n",
        "    dx = np.zeros((n_x, m, T_x))\n",
        "    da0 = np.zeros((n_a, m))\n",
        "    da_prevt = np.zeros((n_a, m))\n",
        "    dc_prevt = np.zeros((n_a, m))\n",
        "    dWf = np.zeros((n_a, n_a + n_x))\n",
        "    dWi = np.zeros((n_a, n_a + n_x))\n",
        "    dWc = np.zeros((n_a, n_a + n_x))\n",
        "    dWo = np.zeros((n_a, n_a + n_x))\n",
        "    dbf = np.zeros((n_a, 1))\n",
        "    dbi = np.zeros((n_a, 1))\n",
        "    dbc = np.zeros((n_a, 1))\n",
        "    dbo = np.zeros((n_a, 1))\n",
        "\n",
        "    # Loop through all time steps in reverse\n",
        "    for t in reversed(range(T_x)):\n",
        "        gradients = lstm_cell_backward(da[:,:,t] + da_prevt, dc_prevt, caches_list[t])\n",
        "\n",
        "        dx[:,:,t] = gradients[\"dxt\"]\n",
        "        dWf += gradients[\"dWf\"]\n",
        "        dWi += gradients[\"dWi\"]\n",
        "        dWc += gradients[\"dWc\"]\n",
        "        dWo += gradients[\"dWo\"]\n",
        "        dbf += gradients[\"dbf\"]\n",
        "        dbi += gradients[\"dbi\"]\n",
        "        dbc += gradients[\"dbc\"]\n",
        "        dbo += gradients[\"dbo\"]\n",
        "\n",
        "        da_prevt = gradients[\"da_prev\"]\n",
        "        dc_prevt = gradients[\"dc_prev\"]\n",
        "\n",
        "    da0 = da_prevt\n",
        "\n",
        "    # Return all gradients in a dictionary\n",
        "    gradients = {\n",
        "        \"dx\": dx,\n",
        "        \"da0\": da0,\n",
        "        \"dWf\": dWf, \"dbf\": dbf,\n",
        "        \"dWi\": dWi, \"dbi\": dbi,\n",
        "        \"dWc\": dWc, \"dbc\": dbc,\n",
        "        \"dWo\": dWo, \"dbo\": dbo\n",
        "    }\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "2WPomVL1ILgW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Use"
      ],
      "metadata": {
        "id": "-hAYN_BhJJMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Dummy data\n",
        "np.random.seed(1)\n",
        "n_x, n_a, m, T_x = 3, 5, 10, 7\n",
        "\n",
        "x = np.random.randn(n_x, m, T_x)\n",
        "a0 = np.random.randn(n_a, m)\n",
        "\n",
        "parameters = {\n",
        "    \"Wf\": np.random.randn(n_a, n_a + n_x),\n",
        "    \"bf\": np.random.randn(n_a, 1),\n",
        "    \"Wi\": np.random.randn(n_a, n_a + n_x),\n",
        "    \"bi\": np.random.randn(n_a, 1),\n",
        "    \"Wc\": np.random.randn(n_a, n_a + n_x),\n",
        "    \"bc\": np.random.randn(n_a, 1),\n",
        "    \"Wo\": np.random.randn(n_a, n_a + n_x),\n",
        "    \"bo\": np.random.randn(n_a, 1),\n",
        "    \"Wy\": np.random.randn(n_x, n_a),\n",
        "    \"by\": np.random.randn(n_x, 1),\n",
        "}\n",
        "\n",
        "# Forward pass\n",
        "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
        "print(\"a.shape:\", a.shape)\n",
        "print(\"y.shape:\", y.shape)\n",
        "print(\"c.shape:\", c.shape)\n",
        "\n",
        "# Backward pass\n",
        "da = np.random.randn(*a.shape)\n",
        "gradients = lstm_backward(da, caches)\n",
        "print(\"dx.shape:\", gradients[\"dx\"].shape)\n",
        "print(\"da0.shape:\", gradients[\"da0\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4YPadCBJInt",
        "outputId": "3014790a-a049-45da-e518-7c3c6b44609c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.shape: (5, 10, 7)\n",
            "y.shape: (3, 10, 7)\n",
            "c.shape: (5, 10, 7)\n",
            "dx.shape: (3, 10, 7)\n",
            "da0.shape: (5, 10)\n"
          ]
        }
      ]
    }
  ]
}